{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the model from model.onnx to resnes_engine.trt:\n",
    "!trtexec.exe --onnx=best2.onnx --saveEngine=best2.trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8402] # trtexec --onnx=best2.onnx --int8 --workspace=64 --buildOnly --saveEngine=best2.engine"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/12/2023-03:08:58] [W] --workspace flag has been deprecated by --memPoolSize flag.\n",
      "[03/12/2023-03:09:00] [W] [TRT] onnx2trt_utils.cpp:369: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[03/12/2023-03:09:00] [W] [TRT] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.\n",
      "[03/12/2023-03:09:26] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[03/12/2023-03:09:26] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[03/12/2023-03:08:58] [I] === Model Options ===\n",
      "[03/12/2023-03:08:58] [I] Format: ONNX\n",
      "[03/12/2023-03:08:58] [I] Model: best2.onnx\n",
      "[03/12/2023-03:08:58] [I] Output:\n",
      "[03/12/2023-03:08:58] [I] === Build Options ===\n",
      "[03/12/2023-03:08:58] [I] Max batch: explicit batch\n",
      "[03/12/2023-03:08:58] [I] Memory Pools: workspace: 64 MiB, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[03/12/2023-03:08:58] [I] minTiming: 1\n",
      "[03/12/2023-03:08:58] [I] avgTiming: 8\n",
      "[03/12/2023-03:08:58] [I] Precision: FP32+INT8\n",
      "[03/12/2023-03:08:58] [I] LayerPrecisions: \n",
      "[03/12/2023-03:08:58] [I] Calibration: Dynamic\n",
      "[03/12/2023-03:08:58] [I] Refit: Disabled\n",
      "[03/12/2023-03:08:58] [I] Sparsity: Disabled\n",
      "[03/12/2023-03:08:58] [I] Safe mode: Disabled\n",
      "[03/12/2023-03:08:58] [I] DirectIO mode: Disabled\n",
      "[03/12/2023-03:08:58] [I] Restricted mode: Disabled\n",
      "[03/12/2023-03:08:58] [I] Build only: Enabled\n",
      "[03/12/2023-03:08:58] [I] Save engine: best2.engine\n",
      "[03/12/2023-03:08:58] [I] Load engine: \n",
      "[03/12/2023-03:08:58] [I] Profiling verbosity: 0\n",
      "[03/12/2023-03:08:58] [I] Tactic sources: Using default tactic sources\n",
      "[03/12/2023-03:08:58] [I] timingCacheMode: local\n",
      "[03/12/2023-03:08:58] [I] timingCacheFile: \n",
      "[03/12/2023-03:08:58] [I] Input(s)s format: fp32:CHW\n",
      "[03/12/2023-03:08:58] [I] Output(s)s format: fp32:CHW\n",
      "[03/12/2023-03:08:58] [I] Input build shapes: model\n",
      "[03/12/2023-03:08:58] [I] Input calibration shapes: model\n",
      "[03/12/2023-03:08:58] [I] === System Options ===\n",
      "[03/12/2023-03:08:58] [I] Device: 0\n",
      "[03/12/2023-03:08:58] [I] DLACore: \n",
      "[03/12/2023-03:08:58] [I] Plugins:\n",
      "[03/12/2023-03:08:58] [I] === Inference Options ===\n",
      "[03/12/2023-03:08:58] [I] Batch: Explicit\n",
      "[03/12/2023-03:08:58] [I] Input inference shapes: model\n",
      "[03/12/2023-03:08:58] [I] Iterations: 10\n",
      "[03/12/2023-03:08:58] [I] Duration: 3s (+ 200ms warm up)\n",
      "[03/12/2023-03:08:58] [I] Sleep time: 0ms\n",
      "[03/12/2023-03:08:58] [I] Idle time: 0ms\n",
      "[03/12/2023-03:08:58] [I] Streams: 1\n",
      "[03/12/2023-03:08:58] [I] ExposeDMA: Disabled\n",
      "[03/12/2023-03:08:58] [I] Data transfers: Enabled\n",
      "[03/12/2023-03:08:58] [I] Spin-wait: Disabled\n",
      "[03/12/2023-03:08:58] [I] Multithreading: Disabled\n",
      "[03/12/2023-03:08:58] [I] CUDA Graph: Disabled\n",
      "[03/12/2023-03:08:58] [I] Separate profiling: Disabled\n",
      "[03/12/2023-03:08:58] [I] Time Deserialize: Disabled\n",
      "[03/12/2023-03:08:58] [I] Time Refit: Disabled\n",
      "[03/12/2023-03:08:58] [I] Inputs:\n",
      "[03/12/2023-03:08:58] [I] === Reporting Options ===\n",
      "[03/12/2023-03:08:58] [I] Verbose: Disabled\n",
      "[03/12/2023-03:08:58] [I] Averages: 10 inferences\n",
      "[03/12/2023-03:08:58] [I] Percentile: 99\n",
      "[03/12/2023-03:08:58] [I] Dump refittable layers:Disabled\n",
      "[03/12/2023-03:08:58] [I] Dump output: Disabled\n",
      "[03/12/2023-03:08:58] [I] Profile: Disabled\n",
      "[03/12/2023-03:08:58] [I] Export timing to JSON file: \n",
      "[03/12/2023-03:08:58] [I] Export output to JSON file: \n",
      "[03/12/2023-03:08:58] [I] Export profile to JSON file: \n",
      "[03/12/2023-03:08:58] [I] \n",
      "[03/12/2023-03:08:58] [I] === Device Information ===\n",
      "[03/12/2023-03:08:58] [I] Selected Device: NVIDIA GeForce RTX 2060 SUPER\n",
      "[03/12/2023-03:08:58] [I] Compute Capability: 7.5\n",
      "[03/12/2023-03:08:58] [I] SMs: 34\n",
      "[03/12/2023-03:08:58] [I] Compute Clock Rate: 1.65 GHz\n",
      "[03/12/2023-03:08:58] [I] Device Global Memory: 8191 MiB\n",
      "[03/12/2023-03:08:58] [I] Shared Memory per SM: 64 KiB\n",
      "[03/12/2023-03:08:58] [I] Memory Bus Width: 256 bits (ECC disabled)\n",
      "[03/12/2023-03:08:58] [I] Memory Clock Rate: 7.001 GHz\n",
      "[03/12/2023-03:08:58] [I] \n",
      "[03/12/2023-03:08:58] [I] TensorRT version: 8.4.2\n",
      "[03/12/2023-03:08:59] [I] [TRT] [MemUsageChange] Init CUDA: CPU +403, GPU +0, now: CPU 9921, GPU 1152 (MiB)\n",
      "[03/12/2023-03:09:00] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +213, GPU +68, now: CPU 10319, GPU 1220 (MiB)\n",
      "[03/12/2023-03:09:00] [I] Start parsing network model\n",
      "[03/12/2023-03:09:00] [I] [TRT] ----------------------------------------------------------------\n",
      "[03/12/2023-03:09:00] [I] [TRT] Input filename:   best2.onnx\n",
      "[03/12/2023-03:09:00] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[03/12/2023-03:09:00] [I] [TRT] Opset version:    13\n",
      "[03/12/2023-03:09:00] [I] [TRT] Producer name:    tf2onnx\n",
      "[03/12/2023-03:09:00] [I] [TRT] Producer version: 1.12.1 b6d590\n",
      "[03/12/2023-03:09:00] [I] [TRT] Domain:           \n",
      "[03/12/2023-03:09:00] [I] [TRT] Model version:    0\n",
      "[03/12/2023-03:09:00] [I] [TRT] Doc string:       \n",
      "[03/12/2023-03:09:00] [I] [TRT] ----------------------------------------------------------------\n",
      "[03/12/2023-03:09:00] [I] Finish parsing network model\n",
      "[03/12/2023-03:09:00] [I] FP32 and INT8 precisions have been specified - more performance might be enabled by additionally specifying --fp16 or --best\n",
      "[03/12/2023-03:09:00] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +566, GPU +216, now: CPU 10846, GPU 1436 (MiB)\n",
      "[03/12/2023-03:09:00] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +111, GPU +50, now: CPU 10957, GPU 1486 (MiB)\n",
      "[03/12/2023-03:09:00] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[03/12/2023-03:09:06] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.\n",
      "[03/12/2023-03:09:26] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[03/12/2023-03:09:26] [I] [TRT] Total Host Persistent Memory: 98496\n",
      "[03/12/2023-03:09:26] [I] [TRT] Total Device Persistent Memory: 2191360\n",
      "[03/12/2023-03:09:26] [I] [TRT] Total Scratch Memory: 0\n",
      "[03/12/2023-03:09:26] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 25 MiB, GPU 71 MiB\n",
      "[03/12/2023-03:09:26] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 5.6786ms to assign 7 blocks to 80 nodes requiring 11722752 bytes.\n",
      "[03/12/2023-03:09:26] [I] [TRT] Total Activation Memory: 11722752\n",
      "[03/12/2023-03:09:26] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +23, GPU +26, now: CPU 23, GPU 26 (MiB)\n",
      "[03/12/2023-03:09:26] [I] Engine built in 27.845 sec.\n",
      "[03/12/2023-03:09:26] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 11429, GPU 1594 (MiB)\n",
      "[03/12/2023-03:09:26] [I] [TRT] Loaded engine size: 23 MiB\n",
      "[03/12/2023-03:09:26] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +25, now: CPU 0, GPU 25 (MiB)\n",
      "[03/12/2023-03:09:26] [I] Engine deserialized in 0.0069445 sec.\n",
      "[03/12/2023-03:09:26] [I] Skipped inference phase since --buildOnly is added.\n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8402] # trtexec --onnx=best2.onnx --int8 --workspace=64 --buildOnly --saveEngine=best2.engine\n"
     ]
    }
   ],
   "source": [
    "#save to .engine model\n",
    "!trtexec --onnx=best2.onnx --int8 --workspace=64 --buildOnly --saveEngine=best2.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8402] # trtexec --onnx=best2.onnx --fp16 --workspace=16 --minShapes=input:1x3x384x384 --optShapes=input:1x3x1026x1282 --maxShapes=input:1x3x1440x2560 --buildOnly --saveEngine=best2.engine\n",
      "[03/12/2023-03:07:46] [I] === Model Options ===\n",
      "[03/12/2023-03:07:46] [I] Format: ONNX\n",
      "[03/12/2023-03:07:46] [I] Model: best2.onnx\n",
      "[03/12/2023-03:07:46] [I] Output:\n",
      "[03/12/2023-03:07:46] [I] === Build Options ===\n",
      "[03/12/2023-03:07:46] [I] Max batch: explicit batch\n",
      "[03/12/2023-03:07:46] [I] Memory Pools: workspace: 16 MiB, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[03/12/2023-03:07:46] [I] minTiming: 1\n",
      "[03/12/2023-03:07:46] [I] avgTiming: 8\n",
      "[03/12/2023-03:07:46] [I] Precision: FP32+FP16\n",
      "[03/12/2023-03:07:46] [I] LayerPrecisions: \n",
      "[03/12/2023-03:07:46] [I] Calibration: \n",
      "[03/12/2023-03:07:46] [I] Refit: Disabled\n",
      "[03/12/2023-03:07:46] [I] Sparsity: Disabled\n",
      "[03/12/2023-03:07:46] [I] Safe mode: Disabled\n",
      "[03/12/2023-03:07:46] [I] DirectIO mode: Disabled\n",
      "[03/12/2023-03:07:46] [I] Restricted mode: Disabled\n",
      "[03/12/2023-03:07:46] [I] Build only: Enabled\n",
      "[03/12/2023-03:07:46] [I] Save engine: best2.engine\n",
      "[03/12/2023-03:07:46] [I] Load engine: \n",
      "[03/12/2023-03:07:46] [I] Profiling verbosity: 0\n",
      "[03/12/2023-03:07:46] [I] Tactic sources: Using default tactic sources\n",
      "[03/12/2023-03:07:46] [I] timingCacheMode: local\n",
      "[03/12/2023-03:07:46] [I] timingCacheFile: \n",
      "[03/12/2023-03:07:46] [I] Input(s)s format: fp32:CHW\n",
      "[03/12/2023-03:07:46] [I] Output(s)s format: fp32:CHW\n",
      "[03/12/2023-03:07:46] [I] Input build shape: input=1x3x384x384+1x3x1026x1282+1x3x1440x2560\n",
      "[03/12/2023-03:07:46] [I] Input calibration shapes: model\n",
      "[03/12/2023-03:07:46] [I] === System Options ===\n",
      "[03/12/2023-03:07:46] [I] Device: 0\n",
      "[03/12/2023-03:07:46] [I] DLACore: \n",
      "[03/12/2023-03:07:46] [I] Plugins:\n",
      "[03/12/2023-03:07:46] [I] === Inference Options ===\n",
      "[03/12/2023-03:07:46] [I] Batch: Explicit\n",
      "[03/12/2023-03:07:46] [I] Input inference shape: input=1x3x1026x1282\n",
      "[03/12/2023-03:07:46] [I] Iterations: 10\n",
      "[03/12/2023-03:07:46] [I] Duration: 3s (+ 200ms warm up)\n",
      "[03/12/2023-03:07:46] [I] Sleep time: 0ms\n",
      "[03/12/2023-03:07:46] [I] Idle time: 0ms\n",
      "[03/12/2023-03:07:46] [I] Streams: 1\n",
      "[03/12/2023-03:07:46] [I] ExposeDMA: Disabled\n",
      "[03/12/2023-03:07:46] [I] Data transfers: Enabled\n",
      "[03/12/2023-03:07:46] [I] Spin-wait: Disabled\n",
      "[03/12/2023-03:07:46] [I] Multithreading: Disabled\n",
      "[03/12/2023-03:07:46] [I] CUDA Graph: Disabled\n",
      "[03/12/2023-03:07:46] [I] Separate profiling: Disabled\n",
      "[03/12/2023-03:07:46] [I] Time Deserialize: Disabled\n",
      "[03/12/2023-03:07:46] [I] Time Refit: Disabled\n",
      "[03/12/2023-03:07:46] [I] Inputs:\n",
      "[03/12/2023-03:07:46] [I] === Reporting Options ===\n",
      "[03/12/2023-03:07:46] [I] Verbose: Disabled\n",
      "[03/12/2023-03:07:46] [I] Averages: 10 inferences\n",
      "[03/12/2023-03:07:46] [I] Percentile: 99\n",
      "[03/12/2023-03:07:46] [I] Dump refittable layers:Disabled\n",
      "[03/12/2023-03:07:46] [I] Dump output: Disabled\n",
      "[03/12/2023-03:07:46] [I] Profile: Disabled\n",
      "[03/12/2023-03:07:46] [I] Export timing to JSON file: \n",
      "[03/12/2023-03:07:46] [I] Export output to JSON file: \n",
      "[03/12/2023-03:07:46] [I] Export profile to JSON file: \n",
      "[03/12/2023-03:07:46] [I] \n",
      "[03/12/2023-03:07:46] [I] === Device Information ===\n",
      "[03/12/2023-03:07:46] [I] Selected Device: NVIDIA GeForce RTX 2060 SUPER\n",
      "[03/12/2023-03:07:46] [I] Compute Capability: 7.5\n",
      "[03/12/2023-03:07:46] [I] SMs: 34\n",
      "[03/12/2023-03:07:46] [I] Compute Clock Rate: 1.65 GHz\n",
      "[03/12/2023-03:07:46] [I] Device Global Memory: 8191 MiB\n",
      "[03/12/2023-03:07:46] [I] Shared Memory per SM: 64 KiB\n",
      "[03/12/2023-03:07:46] [I] Memory Bus Width: 256 bits (ECC disabled)\n",
      "[03/12/2023-03:07:46] [I] Memory Clock Rate: 7.001 GHz\n",
      "[03/12/2023-03:07:46] [I] \n",
      "[03/12/2023-03:07:46] [I] TensorRT version: 8.4.2\n",
      "[03/12/2023-03:07:47] [I] [TRT] [MemUsageChange] Init CUDA: CPU +421, GPU +0, now: CPU 10371, GPU 1152 (MiB)\n",
      "[03/12/2023-03:07:47] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +214, GPU +68, now: CPU 10763, GPU 1220 (MiB)\n",
      "[03/12/2023-03:07:47] [I] Start parsing network model\n",
      "[03/12/2023-03:07:47] [I] [TRT] ----------------------------------------------------------------\n",
      "[03/12/2023-03:07:47] [I] [TRT] Input filename:   best2.onnx\n",
      "[03/12/2023-03:07:47] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[03/12/2023-03:07:47] [I] [TRT] Opset version:    13\n",
      "[03/12/2023-03:07:47] [I] [TRT] Producer name:    tf2onnx\n",
      "[03/12/2023-03:07:47] [I] [TRT] Producer version: 1.12.1 b6d590\n",
      "[03/12/2023-03:07:47] [I] [TRT] Domain:           \n",
      "[03/12/2023-03:07:47] [I] [TRT] Model version:    0\n",
      "[03/12/2023-03:07:47] [I] [TRT] Doc string:       \n",
      "[03/12/2023-03:07:47] [I] [TRT] ----------------------------------------------------------------\n",
      "[03/12/2023-03:07:47] [I] Finish parsing network model\n",
      "&&&& FAILED TensorRT.trtexec [TensorRT v8402] # trtexec --onnx=best2.onnx --fp16 --workspace=16 --minShapes=input:1x3x384x384 --optShapes=input:1x3x1026x1282 --maxShapes=input:1x3x1440x2560 --buildOnly --saveEngine=best2.engine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/12/2023-03:07:46] [W] --workspace flag has been deprecated by --memPoolSize flag.\n",
      "[03/12/2023-03:07:47] [W] [TRT] onnx2trt_utils.cpp:369: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[03/12/2023-03:07:47] [E] Cannot find input tensor with name \"input\" in the network inputs! Please make sure the input tensor names are correct.\n",
      "[03/12/2023-03:07:47] [E] Network And Config setup failed\n",
      "[03/12/2023-03:07:47] [E] Building engine failed\n",
      "[03/12/2023-03:07:47] [E] Failed to create engine from model or file.\n",
      "[03/12/2023-03:07:47] [E] Engine set up failed\n"
     ]
    }
   ],
   "source": [
    "#Build TensorRT engine from ONNX using trtexec tool\n",
    "!trtexec --onnx=best2.onnx --fp16 --workspace=16 --minShapes=input:1x3x384x384 --optShapes=input:1x3x1026x1282 --maxShapes=input:1x3x1440x2560 --buildOnly --saveEngine=best2.engine\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
